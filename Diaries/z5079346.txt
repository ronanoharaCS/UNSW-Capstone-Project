Jonathan Chanz5079346COMP3900 - Diary# Questions to be answered- What was planned for this period since the last work diary entry?- What was finished?- What were the main technical and non-technical obstacles and how they were overcome (or what was tried and did not work)- What was not finished, why this happened, how this affects the overall project progress and what adjustments (if any) are needed so the success of the project is not endangered?- What is planned for the next period?# Week 1 - I planned to join a group, however I couldn’t find a group to join in my tutorial, so I messaged around and ended up joining “H17A-harvey-specter1”. - The team were interested in a custom project I had spent much time thinking about, so I created and submitted a ‘Custom Project Proposal’ for the project. - Following this submission, there was concern about the project amongst the group and it was decided that we would do one of the default topics. Being eager to do a custom project, I ended up joining another group ‘W17A-212-Monolith’. # Week 2- I was warmly introduced to the new group and brought up to speed on the project idea and the general approach intended for implementation- We collaborated on devising epic-stories, user-stories and the associated acceptance criteria for each.# Week 3- We planned to have the project proposal complete this week, so we delegated roles for this assessment task - I was responsible with completing the ‘Background’ section, so I spent time researching ‘Existing Systems’ and seeing how our planned project fitted into the broader picture. With this understanding, I helped to write the Problem Statement and completed the Features of Existing System’s Section. # Week 4- This week involved carrying out our first sprint tasks, so that we have something to present next week for Progressive Demo A. - We allocated roles with Lance and myself opting for doing the back-end, and Liam, James and Ronan doing work on the front-end.- I spent most of this week learning about Docker and the GraphQL framework, which Lance was planning to implement in the back-end  # Week 5- This week I started writing a script in Python, to fetch news data from The Guardian’s API. This involved retrieving data, connecting to our PostgreSQL database and then inserting the news data into our database.- I struggled to integrate the code with Docker, as I was still unfamiliar with how it had been implemented. So, Lance added sample data to the database for Progressive Demo A.# Week 6- Lance added an extra table for article content to our database schema and I added news data insertion into this table. - Considered the best way to make the python script and its dependencies, accessible by the rest of the group. I ran into issues trying to do this with Docker, so I decided to use virtual environments instead.- The front-end made a lot of progress this week, but is still relying on hard-coded data to be presented.# Week 7- This week I made the Python script scalable so that other news API’s could be added with ease.- We worked on Retrospective B and I added the goal to be pro-active with maintaining a README file, so that my code could be readily accessible by other group members.- The script also now inserts The Guardian’s pre-defined topics to the topics table in our database. However, as these topics lack specificity, we plan to do topic analysis to derive specific topics. # Week 8- Lance conducted a peer-2-peer learning demo, which I greatly benefited from.- The front end is now connected to the back end!- I started reading up on Topic Analysis and identifying the best approach to implement it. Identifying the Gensim Package as being the best approach for implementing Topic Analysis, I followed the documentation to do so. # Week 9- I made the topic model functional with our database, so that topics were assigned topics in our database. - The major hurdle this week was deriving appropriate topics for the database. For this, I implement a coherence model to aid choosing an appropriate number of topics. I also ran the model numerous times, varying the pre-processing parameters, to help derive appropriate topics. # Week 10- I generated a topic model based on 16,000 articles and it proved to generate appropriate topics. With articles now assigned appropriate topics, the front-end was effectively presenting real and interesting data.- Helped to create the Final Report and spent time preparing slides and examples for the presentation.- Also, in terms of my ‘GitHub footprint’, the topic analysis files I created were manually uploaded by another team member and hence, were not recorded.